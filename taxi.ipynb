{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent:\n",
    "    def __init__(self, max_episodes, max_steps, test_episodes, alpha, gamma, epsilon, decay_rate):\n",
    "        self.env = gym.make(\"Taxi-v3\", render_mode = \"rgb_array\")\n",
    "        self.qtable = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.max_episodes = max_episodes\n",
    "        self.test_episodes= test_episodes\n",
    "        self.steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_arr = np.empty(max_episodes)\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        print(\"Training....\")\n",
    "        for episode in range(self.max_episodes):\n",
    "#             print(\"Episode: {}\".format(episode))\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            eps_reward = 0\n",
    "            \n",
    "            for step in range(self.steps):\n",
    "                exp_exp_tradeoff = random.uniform(0,1)\n",
    "                if exp_exp_tradeoff > self.epsilon:\n",
    "                    if type(state) == tuple:\n",
    "                        action = np.argmax(self.qtable[state[0],:])\n",
    "                    else:\n",
    "                        action = np.argmax(self.qtable[state,:])\n",
    "                \n",
    "                else:\n",
    "                    action = self.env.action_space.sample()\n",
    "                \n",
    "                new_state, reward, done, info, _ = self.env.step(action)\n",
    "                eps_reward += reward\n",
    "#                 print(\"reward: {}\".format(reward))\n",
    "                if type(state) == tuple:\n",
    "                    self.qtable[state[0], action] = self.qtable[state[0], action] + self.alpha * (reward + self.gamma * \n",
    "                                            np.max(self.qtable[new_state, :]) - self.qtable[state[0], action])\n",
    "                else:\n",
    "                    self.qtable[state, action] = self.qtable[state, action] + self.alpha * (reward + self.gamma * \n",
    "                                            np.max(self.qtable[new_state, :]) - self.qtable[state, action])\n",
    "                    \n",
    "                state = new_state\n",
    "\n",
    "                if done == True: \n",
    "                    self.reward_arr[episode] = eps_reward\n",
    "                    eps_reward = 0\n",
    "                    break\n",
    "                self.reward_arr[episode] = eps_reward\n",
    "            eps_reward = 0\n",
    "        \n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "            self.epsilon = 0.01 + (1.0 - 0.01)*np.exp(-self.decay_rate*episode) \n",
    "            \n",
    "        print(\"Learning done!\")\n",
    "        \n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "    def plot_rewards(self):\n",
    "\n",
    "        plt.title(\"Total Reward per episodes\")\n",
    "        plt.xlabel(\"episode\")\n",
    "        plt.ylabel(\"reward\")\n",
    "        plt.ylim(-400, 20)\n",
    "        plt.grid()\n",
    "        plt.plot(self.reward_arr, \"-r\")\n",
    "        plt.show()     \n",
    "        \n",
    "    def info_rewards(self):\n",
    "        print(\"Max reward: {}\".format(max(self.reward_arr)))\n",
    "        print(\"Min reward: {}\".format(min(self.reward_arr)))\n",
    "        print(\"Avg reward: {}\".format(sum(self.reward_arr)/self.max_episodes))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Q_Agent(max_episodes = 5000, \n",
    "                max_steps = 100, \n",
    "                test_episodes = 1000, \n",
    "                alpha = 0.1, \n",
    "                gamma = 0.95, \n",
    "                epsilon = 1.0, \n",
    "                decay_rate = 0.01)\n",
    "agent.run()\n",
    "agent.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a958dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    \n",
    "     def __init__(self, max_episodes, max_steps, test_episodes, alpha, gamma, epsilon, decay_rate):\n",
    "        self.env = gym.make(\"Taxi-v3\", render_mode = \"rgb_array\")\n",
    "        self.qtable = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.max_episodes = max_episodes\n",
    "        self.test_episodes= test_episodes\n",
    "        self.steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_arr = np.empty(max_episodes)\n",
    "        self.decay_rate = decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30bb65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d49146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6cde3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c97613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
